#![feature(adt_const_params, generic_const_exprs)]

use rand::random;
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use serde_json;
use std::fmt::{self, Display, Formatter};
use std::{f64, result};

/// Determine types of activation functions contained in this module.
#[derive(Debug, Serialize, Deserialize, Clone)]
pub enum ActivationType {
    Sigmoid,
    Tanh,
    Relu,
}

pub fn sigm(x: f64) -> f64 {
    1.0 / (1.0 + x.exp())
}
pub fn der_sigm(x: f64) -> f64 {
    sigm(x) * (1.0 - sigm(x))
}

pub fn tanh(x: f64) -> f64 {
    x.tanh()
}

pub fn der_tanh(x: f64) -> f64 {
    1.0 - x.tanh().powi(2)
}

pub fn relu(x: f64) -> f64 {
    f64::max(0.0, x)
}

pub fn der_relu(x: f64) -> f64 {
    if x <= 0.0 {
        0.0
    } else {
        1.0
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
struct Matrix<const ROWS: usize, const COLS: usize> {
    data: [f64; ROWS * COLS],
    size: usize,
    rows: usize,
    cols: usize,
}

impl<const ROWS: usize, const COLS: usize> Matrix<ROWS, COLS> {
    fn new() -> Self {
        debug_assert!(ROWS > 0 && COLS > 0);
        Matrix {
            size: ROWS * COLS,
            rows: ROWS,
            cols: COLS,
            data: vec![0.0; ROWS * COLS],
        }
    }

    fn from_arr(vec: [f64; ROWS * COLS], rows: usize, cols: usize) -> Self {
        debug_assert!(ROWS > 0 && COLS > 0);
        Matrix {
            size: ROWS * COLS,
            rows: ROWS,
            cols: COLS,
            data: vec,
        }
    }

    fn new_identity(&self) -> Self {
        debug_assert!(ROWS > 0 && COLS == ROWS);
        for i in 0..(ROWS * COLS) {
            self.data[i * COLS + i] = 1.0;
        }

        Matrix {
            size: ROWS * COLS,
            rows: ROWS,
            cols: COLS,
            data: self.data,
        }
    }

    fn get(&self, row: usize, col: usize) -> f64 {
        debug_assert!(row < ROWS && col < COLS && row > 0 && col > 0);
        self.data[row * col + col]
    }

    fn set(&mut self, row: usize, col: usize, value: f64) {
        debug_assert!(row < ROWS && col < COLS && row > 0 && col > 0);
        self.data[row * col + col] = value;
    }

    const fn rows(&self) -> usize {
        self.rows
    }

    const fn cols(&self) -> usize {
        self.cols
    }

    fn dot(&self, other: &Self) -> Matrix<ROWS, COLS> {
        debug_assert!(COLS == other.rows());

        let mut result: Matrix<ROWS, COLS> = Matrix::new();

        for i in 0..self.rows() {
            for j in 0..other.cols() {
                let mut sum = 0.0;
                for k in 0..self.cols() {
                    sum += self.get(i, k) * other.get(k, j);
                }
                result.set(i, j, sum);
            }
        }
        result

        // let n = other.cols();

        // for j in 0..n {
        //     for i in 0..n {
        //         for k in 0..n {
        //             result.set(i, j, result.get(i, j) + self.get(i, k) * other.get(k, j));
        //         }
        //     }
        // }
        // result
    }

    fn dot_vec(&self, other: &[f64; COLS]) -> [f64; ROWS] {
        debug_assert!(self.cols() == other.len());

        let mut result = [0.0; ROWS];
        for i in 0..self.rows() {
            for j in 0..self.cols() {
                result[i] += self.get(i, j) * other[j];
            }
        }
        result
    }

    fn add(&self, other: &Self) -> Matrix<ROWS, COLS> {
        debug_assert!(self.rows() == other.rows() && self.cols() == other.cols());

        let mut result = Matrix::new();
        for i in 0..self.rows() {
            for j in 0..self.cols() {
                result.set(i, j, self.get(i, j) + other.get(i, j));
            }
        }
        result
    }

    fn get_value(&self) -> Option<f64> {
        if self.rows() == 1 && self.cols() == 1 {
            Some(self.get(0, 0))
        } else {
            None
        }
    }
}

// impl Display for Matrix<ROWS, COLS> {
//     fn fmt(&self, f: &mut Formatter) -> fmt::Result {
//         let mut output = String::new();
//         for i in 0..self.rows() {
//             for j in 0..self.cols() {
//                 output.push_str(&format!("{} ", self.get(i, j)));
//             }
//             output.push_str("\n");
//         }
//         write!(f, "{}", output)
//     }
// }

// #[derive(Debug, Serialize, Deserialize)]
// struct Neuron {
//     weights: Vec<f64>,
//     bias: f64,
// }

// impl Neuron {
//     fn new_from(weights: Vec<f64>, bias: f64) -> Self {
//         Neuron { weights, bias }
//     }

//     fn new(input_size: usize) -> Self {
//         let weights = (0..input_size)
//             .into_par_iter()
//             .map(|_| random::<f64>())
//             .collect();
//         Neuron {
//             weights,
//             bias: random::<f64>(),
//         }
//     }

//     fn forward_propagate(&self, input: &Vec<f64>) -> f64 {
//         let mut output = 0.0;
//         for (i, weight) in self.weights.iter().enumerate() {
//             output += input[i] * weight;
//         }
//         output + self.bias
//     }
// }

// #[derive(Debug, Serialize, Deserialize, Clone)]
// struct Layer<const SIZE: usize> {
//     size: usize,
//     bias: [f64; SIZE],
// }

// impl<const SIZE: usize> Layer<SIZE> {
//     fn new(size: usize) -> Self {
//         Layer {
//             size,
//             bias: (0..size).into_par_iter().map(|_| random::<f64>()).collect(),
//         }
//     }
// }

#[derive(Debug, Serialize, Deserialize, Clone)]
struct Network<const INPUT_SIZE: usize, const OUTPUT_SIZE: usize, const HIDDEN_LAYERS: usize> {
    inputs: usize,                         // number of neurons in input layer
    outputs: usize,                        // number of neurons in output layer
    hidden_layers: [usize; HIDDEN_LAYERS], // number of hidden layers (each layer has a number of neurons)
    layer_matrices: Vec<(Matrix<ROWS', COLS>, Matrix<ROWS, COLS>)>, // (weights, biases)
    activation: ActivationType, // activation function
}

impl Network {
    fn new(inputs: usize, outputs: usize, hidden_layers: Vec<Layer>) -> Self {
        Network {
            inputs: Layer::new(inputs),
            outputs: Layer::new(outputs),
            hidden_layers,
            layer_matrices: vec![],
            activation: ActivationType::Sigmoid,
        }
    }

    fn empty_network(inputs: usize, outputs: usize) -> Self {
        Network {
            inputs: Layer::new(inputs),
            outputs: Layer::new(outputs),
            hidden_layers: vec![],
            layer_matrices: vec![],
            activation: ActivationType::Sigmoid,
        }
    }

    fn add_hidden_layer(&mut self, layer: Layer) {
        self.hidden_layers.push(layer);
    }

    fn add_hidden_layer_with_size(&mut self, size: usize) {
        self.hidden_layers.push(Layer::new(size));
    }

    fn compile(&mut self) {
        let mut layers = vec![self.inputs.clone()];
        layers.append(&mut self.hidden_layers.clone());
        layers.push(self.outputs.clone());

        for i in 0..layers.len() - 1 {
            let mut weights = vec![];
            let mut biases = vec![];
            for _ in 0..layers[i + 1].size {
                weights.append(
                    &mut (0..layers[i].size)
                        .into_par_iter()
                        .map(|_| random::<f64>())
                        .collect(),
                );
                biases.push(random::<f64>());
            }
            let weights = Matrix::from_vec(weights, layers[i + 1].size, layers[i].size);
            let biases = Matrix::from_vec(biases, layers[i + 1].size, 1);
            self.layer_matrices.push((weights, biases));
        }
    }

    fn set_activation(&mut self, activation: ActivationType) {
        self.activation = activation;
    }

    fn set_layer_weights(&mut self, layer: usize, weights: Matrix) {
        debug_assert!(layer < self.layer_matrices.len());
        self.layer_matrices[layer].0 = weights;
    }

    fn set_layer_biases(&mut self, layer: usize, biases: Matrix) {
        debug_assert!(layer < self.layer_matrices.len());
        self.layer_matrices[layer].1 = biases;
    }

    // fn forward_propagate(&self, input: &Vec<f64>) -> Vec<f64> {
    //     let (weights, biases) = &self.layer_matrices[0];
    //     let mut output: Vec<f64> = weights.dot_vec(input);
    //     output = output.add(&biases);

    //     for i in 1..self.layer_matrices.len() {
    //         let (weights, biases) = &self.layer_matrices[i];

    //         layer_output = weights.dot(&layer_output);
    //         layer_output = layer_output.add(biases);
    //         match self.activation {
    //             ActivationType::Sigmoid => {
    //                 layer_output = Matrix::from_vec(
    //                     layer_output.data.into_par_iter().map(|x| sigm(x)).collect(),
    //                     input.len(),
    //                     1,
    //                 );
    //             }
    //             ActivationType::Tanh => {
    //                 layer_output = Matrix::from_vec(
    //                     layer_output.data.into_par_iter().map(|x| tanh(x)).collect(),
    //                     input.len(),
    //                     1,
    //                 );
    //             }
    //             ActivationType::Relu => {
    //                 layer_output = Matrix::from_vec(
    //                     layer_output.data.into_par_iter().map(|x| relu(x)).collect(),
    //                     input.len(),
    //                     1,
    //                 );
    //             }
    //         }
    //     }

    //     output
    // }
}

fn main() {
    // let mut network = Network::empty_network(3, 1);

    // network.add_hidden_layer_with_size(4);
    // network.add_hidden_layer_with_size(4);
    // network.compile();

    // let layer_1_weights = Matrix::from_vec(
    //     vec![
    //         0.03, 0.62, 0.85, 0.60, 0.62, 0.64, 0.75, 0.73, 0.34, 0.46, 0.14, 0.06,
    //     ],
    //     4,
    //     3,
    // );
    // let layer_1_biases = Matrix::from_vec(vec![0.14, 0.90, 0.65, 0.32], 4, 1);
    // let layer_2_weights = Matrix::from_vec(
    //     vec![
    //         0.90, 0.95, 0.26, 0.70, 0.12, 0.84, 0.58, 0.78, 0.92, 0.16, 0.49, 0.90, 0.64, 0.60,
    //         0.64, 0.85,
    //     ],
    //     4,
    //     4,
    // );
    // let layer_2_biases = Matrix::from_vec(vec![0.41, 0.09, 0.28, 0.70], 4, 1);
    // let layer_3_weights = Matrix::from_vec(vec![0.23, 0.34, 0.24, 0.67], 1, 4);
    // let layer_3_biases = Matrix::from_vec(vec![0.23], 1, 1);

    // network.set_layer_weights(0, layer_1_weights);
    // network.set_layer_biases(0, layer_1_biases);
    // network.set_layer_weights(1, layer_2_weights);
    // network.set_layer_biases(1, layer_2_biases);
    // network.set_layer_weights(2, layer_3_weights);
    // network.set_layer_biases(2, layer_3_biases);

    // let input: Vec<f64> = vec![2., 1., -1.];

    // network.forward_propagate(&input);

    // println!("{:?}", network);
}
